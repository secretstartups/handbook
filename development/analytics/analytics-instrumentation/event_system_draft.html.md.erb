---
layout: handbook-page-toc
title: Analytics Instrumentation Group Event System Draft
description: "The Analytics Instrumentation draft documentation for new proposed event system"
---

## On this page
{:.no_toc .hidden-md .hidden-lg}

- TOC
{:toc .hidden-md .hidden-lg}

Disclaimer: This is draft documentation to inform a potential iteration of Gitlab's internal tooling
{: .alert .alert-info}

# GitLab Internal Events Tracking

The Usage Insights system is backward compatible with legacy Analytics Instrumentation tooling, and all
preexisting data reported from legacy systems are still going to be reported as it was. For more information about
legacy systems and migration follow to [later section](#migration-from-legacy-systems-service-ping-snowplow) of this documentation.
{: .alert .alert-info}

Analytics Instrumentation events provide the ability to record information about different actions occurring in a GitLab installation.
This documentation is structured in such a way that most readers can add metrics to their code by following the [Quick Start Guide](#quick-start-guide).

The Quick Start Guide section is followed by [information](#using-usage-insights-data) about the resulting data and how to utilize it to gather product insights to help drive business decisions.

The two main elements in the system are events and metrics.

## Event

Everything starts with an event. An event can be any action happening in a system.
Such actions might be initiated by user interaction like displaying a Merge Request page
or hovering the mouse cursor over the top navigation search.
Other actions can result from background system processing like scheduled pipeline succeeding,
or receiving API calls from 3rd party system.
Regardless of the sources if some action is useful for drawing some insights and helping make more educated business decisions,
that action can produce persistent records holding the information that it has occurred.
The persistent record would be later called an event record,
and besides holding information about some specific activities happening in the system it can contain additional details about
the context (state of GitLab system) that accompanied this action.

## Metric

Events data is only available directly for SaaS (GitLab.com) instance and not on self-managed ones.
Metrics on the other hand are collected on weekly cycle from all Gitlab instances that gave the GitLab organization
their consent to collect data.
{: .alert .alert-info}

The information in a single event is not informative enough and might be caused by a coincidence,
in order to have a robust foundation it is necessary to look for sets of events sharing common traits.
For example, a single event documenting a paid user visiting Merge Request page after a new feature was released tells nothing about the success of this new feature. However, if one counts the number of Merge Request page view event happening in the week before the new feature release and then compares it with the number of events for the week following the feature release, then that person will be able to gauge if the new feature increased popularity of Merge Request page.

Described in the example process of counting events based on common traits in defined time frames
is the process that leads to the creation of a second important entity in Analytics Instrumentation: a metric.
A metric is a piece of Analytics Instrumentation data that results from analyzing sets of event records defined by some common traits and limited to some time frame.

To learn more about all available types of metrics follow to [later section](#metric-types) of documentation.

To see an example that illustrates how a generator can be used to add events and metrics, continue reading to the next paragraph.

## Quick start guide

The quick start guide is going to show how to use the Analytics Instrumentation generator to create the necessary configuration files
(aka [event](#event) and [metric definitions](#metric) explained later in this document)
you need as well as produce the code snippet to add to the GitLab's codebase.
You could create all this by hand, but the generator will be faster in most cases.

Let us imagine a scenario where your team just added a cool new feature where an AI model analyses a Merge Request diff and adds automated code review.
You and your team are very excited and you want to know if users like this new feature as much as you do.
With your team, you agreed that the best measure for success is when a merge request author applies an AI suggestion to their code. So how to go about it?

### Declare an event and a metric

As you chose to use "number of merge request that have applied suggestions" as a success metric, it means that a specific user action has been identified as an important event.

To measure this success metric you need to do three things:
1. Define an [event](#event): `applied_ai_code_review_suggestion`
1. Define one or more [metrics](#metric)
1. Trigger events at the right places in the codebase

By using the generator, you can accomplish 1) and 2). The generator will also give you code samples you can use to accomplish 3).

Let's get started by running the generator `$ bin/pi_generate`:

<details markdown=1><summary>An example of using the generator make an event.</summary>
```shell
$ bin/pi_generate
> Please provide a unique name for your event. Only lowercase alphanumeric characters and underscore is allowed for use. The Analytics Instrumentation team recommends following pattern <action>_<target>.

applied_ai_code_review_suggestion

> Please enter the URL of the issue you are working on. We will try to pull in as much information as we can. You get to review the information afterwards.

https://gitlab.com/gitlab-com/www-gitlab-com/-/issues/1234

> product_group set to: "group::code review" based on the issue
> milestone set to "15.10" based on the issue
> Would your event be only available for paid tiers of GitLab Y/N

Y

> Would your event be only available for the ultimate tier of GitLab Y/N

N

> By convention all events automatically include the following properties:
> * environment: string,
> * source: string (eg: ruby, javascript)
> * user_id: number
> * project_id: number
> * namespace_id: number
> * plan: string (eg: free, premium, ultimate)
> Would you like to remove default properties from the event? Y/N

N

> Would you like to add additional properties to the event? Y/N

Y

> New property name? (e.g. project_id)

merge_request_id

> The merge_request_id property type (number, string, url, boolean):

number

> Is the merge_request_id property required? Y/N

Y

> Would you like to add additional properties to the event? Y/N

Y

> New property name? (e.g. project_id)

merge_request_milestone

> The merge_request_milestone property type (number, string, url, boolean):

string

> Is the merge_request_milestone property required? Y/N

N

> Would you like to add additional properties to the event? Y/N

N

> Please describe in at least 150 characters what this event is supposed to track, where, and when
> your answer will be processed to power a full-text search tool and help others find and reuse this event.
> The Analytics Instrumentation group and others thanks you for your help and collaboration!

It is tracking users who apply code suggestion changes made by an AI model to merge request diff.
It helps the code review group to understand the utilization of the new AI-powered feature.
Additional information that this event includes are project id, merge request id, and milestone
```

</details>

You are now done defining the event. The next steps will guide you through setting up metrics to count the events.

We support several different types of metrics that you can read about [here](#metric). The generator will help you create the
most common ones now:

<details markdown=1><summary>A continuation of the example of generator used to create a metric based on newly added events.</summary>
```shell
> Do you want to measure your new event with a metric? Y/N

Y

> You can either count all the events or the number of distinct attributes within events.
> Here are two examples to explain the difference:
> * Count how many times applied_ai_code_review_suggestion was triggered (count)
> * Count the number of distinct projects for which applied_ai_code_review_suggestion was triggered (distinct)
> Would you like to count the total or distinct number of events? count/distinct

distinct

> Which properties would you like make a distinct count on?
> project_id
> merge_request_id
> merge_request_milestone

merge_request_id

> Which time frames do you want to use (all-time, 7days, 28days)? Select one or more:

7days 28days

> We generated following name for your metric:
> * count_distinct_merge_request_id_from_applied_ai_code_review_suggestion_v1_0_0
>
> This is the last step, please describe in at least 150 characters
> what count_distinct_merge_request_id_from_applied_ai_code_review_suggestion_v1_0_0 metric represents,
> consider mentioning: events, and event attributes in the description.
> your answer will be processed to power a full-text search tool and help others find and reuse this metric.
> The Analytics Instrumentation group thanks you for your help and collaboration!

It is representing all applied code review suggestions that were generated with the usage of the new AI-powered feature.
This metric counts distinct merge request id from applied_ai_code_review_suggestion event records that occurred within different time frames.
```
</details>

That's it. You have now generated an event and a metric. The generator will output some helpful code snippets you can
use to trigger events in your backend or frontend code.

<details markdown=1><summary>A closing output from the generator used in default mode to create both new events and new metric based on newly added events.</summary>
```shell
> All done!

> Generated:
> config/events/definitions/active/applied_ai_code_review_suggestion/1-0-0.json
> config/metrics/definitions/active/count_distinct_merge_request_id_from_applied_ai_code_review_suggestion_v1_0_0.yml
>
> You can track your events in Ruby using:

    GitLab::Tracking.applied_ai_code_review_suggestion_1_0_0(project_id: 1,
                        merge_request_id: 1,
                        namespace_id: namespace.id
                        user_id: current_user.id
                        merge_request_milestone: "string")

> You can track your events in Vue using this Tracking mixin:
      this.track(
        'applied_ai_code_review_suggestion',
        '1-0-0',
        {
          user_id: this.userId,
          namespace_id: this.namespaceId,
          project_id: this.projectId,
          merge_request_id: this.mergeRequestId,
          merge_request_milestone: this.mergeRequestMilestone,
        }
      );

```
</details>

When invoked like above, the generator will create the following event definition at `config/events/definitions/active/applied_ai_code_review_suggestion/1-0-0.json` and metric definition at
`/config/metrics/definitions/active/count_distinct_merge_request_id_from_applied_ai_code_review_suggestion_v1_0_0.yml`

### Declare a metric using an existing event

It might happen that you want to add a new metric that would only use already existing events.
If you want to skip the event creation part in the generator and go straight to a new metric creation you can use the generator in `metric-only` mode.
The following example will show a new metric addition process for a single existing event, however, it is possible to use multiple events as well which would be
discussed in [later section](#multi-event-based-metrics) of this document:

<details markdown=1><summary>An example of generator --metric-only mode to add new metric using existing events.</summary>
```shell
$ bin/pi_generate --metric-only
> Which event would you like to create a metric for?

applied_ai_code_review_suggestion

> Which versions of the metric would you like to include?
> 0-9-0
> 1-0-0

1-0-0

> Please enter the URL of the issue you are working on. We will try to pull in as much information as we can. You get to review the information afterwards.

https://gitlab.com/gitlab-com/www-gitlab-com/-/issues/1234

> product_group set to: "group::code review" based on the issue
> milestone set to "15.10" based on the issue
> Would your event be only available for paid tiers of GitLab Y/N

N

> You can either count all the events or the number of distinct attributes within events.
> Here are two examples to explain the difference:
> * Count how many times applied_ai_code_review_suggestion was triggered (count)
> * Count the number of distinct projects for which applied_ai_code_review_suggestion was triggered (distinct)
> Would you like to count the total or distinct number of events? count/distinct

distinct

> Which properties would you like make a distinct count on?
> project_id
> merge_request_id
> merge_request_milestone

merge_request_id

> Which time frames do you want to use (all-time, 7days, 28days)? Select one or more:

7days 28days

> We generated following name for your metric:
> * count_distinct_merge_request_id_from_applied_ai_code_review_suggestion_v1_0_0
>
> This is the last step, please describe in at least 150 characters
> what count_distinct_merge_request_id_from_applied_ai_code_review_suggestion_v1_0_0 metric represents,
> consider mentioning: events, and event attributes in the description.
> your answer will be processed to power a full-text search tool and help others find and reuse this metric.
> The Analytics Instrumentation group thanks you for your help and collaboration!

It is representing all applied code review suggestions that were generated with the usage of the new AI-powered feature.
This metric counts distinct merge request id from applied_ai_code_review_suggestion event records that occurred within different time frames.

> All done!

> Generated:
> config/events/definitions/active/applied_ai_code_review_suggestion/1-0-0.json
> config/metrics/definitions/active/count_distinct_merge_request_id_from_applied_ai_code_review_suggestion_v1_0_0.yml
```
</details>

After you add the new metric definition your new metric will automatically get updated when the events you included are triggered.

### Event tracking on the frontend

You can track your events using `Tracking` mixin offering a single method `track`.
It takes 3 arguments in following order:
1. `event_name` - a string with the same value as can be found under `self.name` in your event schema. `event_name` is also presented by the generator after successfully event definition creation
1. `event_version` - a string with the same value as can be found under `self.version` in your event schema. `event_version` is also presented by the generator after successfully event definition creation
1. An object with all additional properties according to the event definition file
under `properties.context` with exception of with exception of `environment`, `source`, and `plan` that are filled automatically by SDK and are not exposed as arguments of the tracking method

```vue
--- a/app/assets/javascripts/vue_shared/components/markdown/apply_suggestion.vue
+++ b/app/assets/javascripts/vue_shared/components/markdown/apply_suggestion.vue
@@ -1,9 +1,11 @@
 <script>
 import { GlDropdown, GlDropdownForm, GlFormTextarea, GlButton, GlAlert } from '@gitlab/ui';
 import { __, n__ } from '~/locale';
+import Tracking from '~/tracking';

 export default {
   components: { GlDropdown, GlDropdownForm, GlFormTextarea, GlButton, GlAlert },
+  mixins: [Tracking.mixin()],
   props: {
     disabled: {
       type: Boolean,
@@ -24,6 +26,19 @@ export default {
       required: false,
       default: null,
     },
+    mergeRequestId: {
+      type: Number,
+      required: true,
+    },
+    mergeRequestMilestone: {
+      type: String,
+      required: false,
+      default: null,
+    },
+    projectId: {
+      type: Number,
+      required: true,
+    },
   },
   data() {
     return {
@@ -42,6 +57,13 @@ export default {
   methods: {
     onApply() {
       this.$emit('apply', this.message);
+      this.track(
+        'applied_ai_code_review_suggestion',
+        '1-0-0',
+        {
+          user_id: this.userId,
+          namespace_id: this.namespaceId,
+          project_id: this.projectId,
+          merge_request_id: this.mergeRequestId,
+          merge_request_milestone: this.mergeRequestMilestone,
+        }
+      );
     },
   },
 };
```

### Event tracking on the backend

The Analytics Instrumentation backend SDK will generate dedicated method for each event that should be used for tracking purposes.
In the [Quick start](#quick-start-guide) example Analytics Instrumentation framework will generate method
`GitLab::Tracking.applied_ai_code_review_suggestion_1_0_0(project_id:, merge_request_id:, merge_request_milestone: nil)`
In order to add `applied_ai_code_review_suggestion` event tracking you would need to visit
[`Suggestions::ApplyService`](https://gitlab.com/gitlab-org/gitlab/-/blob/master/app/services/suggestions/apply_service.rb#L34)
and add tracking method call there.

```diff
--- a/app/services/suggestions/apply_service.rb
+++ b/app/services/suggestions/apply_service.rb
@@ -32,10 +32,20 @@ def update_suggestions(result)
       Suggestion.id_in(suggestion_set.suggestions)
         .update_all(commit_id: result[:result], applied: true)

+      GitLab::Tracking.applied_ai_code_review_suggestion_1_0_0(project_id: suggestion_set.source_project.id,,
+                       merge_request_id: merge_request.id,
+                       namespace_id: suggestion_set.source_project.namespace.id,
+                       user_id: current_user.id
+                       merge_request_milestone: merge_request.milestone)
+
       Gitlab::UsageDataCounters::MergeRequestActivityUniqueCounter
         .track_apply_suggestion_action(user: current_user, suggestions: suggestion_set.suggestions)
     end
```

An event tracking method name is generated by appending with underscore  `event_version` at the end of `event_name` and replacing all dashes with underscores.
The generated method keywords arguments match the event properties from the event definition file
under `properties.context` with exception of `environment`, `source`, and `plan` that are filled automatically by SDK
and are not exposed as arguments of the tracking method.
All optional properties would default to `null`

## Event definition

Each event in the GitLab system has its corresponding JSONSchema file that is used for both documentation and validation of incoming event
records to make sure that data is not corrupted due to a bug or adversary action.
Event definition files can be manipulated directly via preferred text editor tool.
However in majority of cases files created by the generator do not require any further user attention.
Below we present example content of an event definition file.

<details markdown=1><summary>An event definition.</summary>

```json
{
  "$schema": "http://iglucentral.com/schemas/com.snowplowanalytics.self-desc/schema/jsonschema/1-0-0#",
  "description": "applied_ai_code_review_suggestion is tracking users who apply code suggestion changes made by an AI model to merge request diff. It helps the code review group to understand the utilization of the new AI-powered feature. Additional information that this event includes are project id, merge request id, and milestone",
  "self": {
    "vendor": "com.gitlab",
    "name": "applied_ai_code_review_suggestion",
    "version": "1-0-0",
    "format": "jsonschema"
  },
  "type": "object",
  "properties": {
    "name": { "const": "applied_ai_code_review_suggestion" },
    "context": {
      "type": "object",
      "properties": {
        "user_id": { "type": "number" },
        "namespace_id": { "type": "number" },
        "project_id": { "type": "number" },
        "merge_request_id": { "type": "number" },
        "merge_request_milestone": { "type": "string" },
        "environment": { "type": "string" },
        "plan": { "type": "string" },
        "source": { "type": "string" }
      },
      "additionalProperties": false,
      "required": [
        "project_id",
        "merge_request_id",
        "environment",
        "source"
      ]
    }
  },
  "product_group": "group::configure",
  "milestone": "15.10",
  "introduced_by_url": "https://gitlab.com/gitlab-org/gitlab/-/merge_requests/82690",
  "availability_limited_to": [
    "premium",
    "ultimate"
  ],
  "additionalProperties": false,
  "required": [
    "name",
    "context"
  ]
}
```
</details>

## Metric definition

Each metric in the GitLab system has its corresponding YAML file that is used for both documentation and validation
to make sure that data is not corrupted due to a bug or adversary action.
Metric definition files can be manipulated directly via preferred text editor tool.
However in majority of cases files created by the generator do not require any further user attention.
Below we present example content of a metric definition file:

<details markdown=1><summary>A metric definition.</summary>
```yaml
---
data_category: optional
name: count_distinct_merge_request_id_from_applied_ai_code_review_suggestion_v1_0_0
description: >
  It is representing all applied code review suggestions that were generated with the usage of the new AI-powered feature.
  This metric counts distinct merge request id from applied_ai_code_review_suggestion event records that occurred within different time frames.
source: events
events:
- applied_ai_code_review_suggestion:
  version: 1-0-0
  unique_on: merge_request_id
filters:
  time_frames: [7days]
value_type: number
performance_indicator_type: []
introduced_by_url: https://gitlab.com/gitlab-com/www-gitlab-com/-/issues/1234
product_group: group::code review
milestone: "15.10"
tier:
- premium
- ultimate
```
</details>

## Metric types

Metrics can use one or more events, and have two two types computation available that will be discussed in following paragraphs.

### Total count metric

The most straightforward type of metric.
It counts all event records for an appointed event that satisfy a set of defined filters.
This type of metric does NOT exclude duplicated entries and it can be compared to running SQL `SELECT COUNT(*) FROM events` statement.

<details markdown=1><summary>An example metric definition.</summary>

```yaml
---
data_category: optional
name: count_event_applied_ai_code_review_suggestion_v1_0_0
description: >
  count_event_applied_ai_code_review_suggestion_v1_0_0 is representing all applied code review suggestions
  that were generated with the usage of new AI-powered feature. This metric counts all applied_ai_code_review_suggestion
  event records that occurred within the last 7 and 28 days.
source: events
events:
- applied_ai_code_review_suggestion:
  version: 1-0-0
filters:
  time_frames: [7days, 28days]
value_type: number
performance_indicator_type: []
introduced_by_url: https://gitlab.com/gitlab-com/www-gitlab-com/-/issues/1234
product_group: "group::code review"
milestone: "15.10"
tier:
- premium
- ultimate
```
</details>

### Distinct count metric

This type of metric is distinguished by `events` having a `unique_on` attribute defined.
It counts unique values of selected (at `events.<event_name>.unique_by: project_id`) event record attribute
based on all event records for the appointed event that satisfy a set of defined filters.
This type of metric excludes duplicated entries and it can be compared to running SQL `SELECT COUNT(DISTINCT project_id) FROM events` statement.

<details markdown=1><summary>An example metric definition.</summary>

```yaml
---
data_category: optional
name: count_distinct_project_id_from_event_applied_ai_code_review_suggestion_v1_0_0
description: >
  count_distinct_project_id_from_event_applied_ai_code_review_suggestion_v1_0_0 is representing
  total number of projects in which code review suggestions generated by AI-powered feature
  were applied. This metric counts unique project_id from applied_ai_code_review_suggestion
  event records that occurred within the last week and last month.
source: events
events:
- applied_ai_code_review_suggestion:
  version: 1-0-0
  unique_on: project_id
filters:
  time_frames: [7days, 28days]
value_type: number
performance_indicator_type: []
introduced_by_url: https://gitlab.com/gitlab-com/www-gitlab-com/-/issues/1234
product_group: "group::code review"
milestone: "15.10"
tier:
- premium
- ultimate
```
</details>

### Multi event based metrics

On some occasions, you might be interested in getting information based on a group of events rather than just a single one.
It is possible to define a metric that will have multiple events listed under `events`.
It is important to make sure that `events.<event_name>.unique_by` attribute for each event represent the same data, for example, user identifier
so the metric will not compare apples with oranges, producing confusing results.

Depending on the type of metric computation (`count` and `count_distinct`), that metric will require different configuration bits
which will be discussed in the following subsections. However, for each of those cases, the Analytics Instrumentation team recommends using a generator that will
guide you through the process.

#### Multi event total count metrics

Total counts based on more than one event counts all event records that was tracked for all listed events. It could be pictured as running SQL
`SELECT (SELECT COUNT(*) FROM event1) + (SELECT COUNT(*) FROM event2);`

A metric definition can be configured with `$ bin/pi_generate` generator

<details markdown=1><summary>An example usage of the generator to create a metric that is based on multiple events.</summary>
```shell
$ bin/pi_generate --metric-only

> You can either count all the events or the number of distinct attributes within events.
> Here are two examples to explain the difference:
> * Count how many times applied_ai_code_review_suggestion was triggered (count)
> * Count the number of distinct projects for which applied_ai_code_review_suggestion was triggered (distinct)
> Would you like to count the total or distinct number of events? count/distinct

count

> Please provide the event name that will be used to compute your metric

applied_ai_code_review_suggestion

> Please provide version of applied_ai_code_review_suggestion event for use

1-0-0

> Do you want to add more events Y/N

Y

> Please next event name that will be used to compute your metric

run_ai_security_scan

> Please provide version of run_ai_security_scan event for use

1-0-1

> Do you want to add more events Y/N

N

> ... Ask about time frame, tier, name and desscription
```
</details>

The generator (after filling rest of the configuration steps discussed in [_Quick Start_](#quick-start-guide) section) would produce
following metric definition:

<details markdown=1><summary>The metric definition.</summary>

```yaml
---
data_category: optional
name: count_total_users_of_ai_features
description: >
  count_total_users_of_ai_features is representing
  total number of event records related to any of the AI-powered features
  that occurred within the last week and last month.
source: events
events:
  - applied_ai_code_review_suggestion:
    version: 1-0-0
  - run_ai_security_scan:
    version: 1-0-1
filters:
  time_frames: [7days, 28days]
value_type: number
performance_indicator_type: []
introduced_by_url: https://gitlab.com/gitlab-com/www-gitlab-com/-/issues/1234
product_group: "group::code review"
milestone: "15.10"
tier:
- premium
- ultimate
```

The above configuration would sum a total number of event records that were recorded for each of the `option.events` listed events within the last week.
</details>

#### Multi event distinct count metrics

Another type of metric that can analyze multiple events at once is _distinct count_. Providing insight into how many unique values
appeared for all event records representing different events.
The multi-event distinct count metric is computed according to the following steps:
1. From all events listed in a metric definition select appointed (at `events.<event_name>.unique_by`) attributes value
1. Compute a distinct count of all values in the data set built at 1st step

The resulting data provide information about a number of unique values of attributes listed in `events.<event_name>.unique_by`
of all events. This kind of data can be useful to get an aggregated overview of a combined utilization of features coming from
a single category.

Example representative SQL query looks as follows:

```SQL
SELECT COUNT(DISTINCT user_id) FROM
(
  SELECT id AS user_id FROM users
  UNION ALL
  SELECT author_id AS user_id FROM issues
) as users;
```

To create a metric definition for multi event distinct count metric you can use `$ bin/pi_generate` generator

<details markdown=1><summary>An example usage of the generator to create a metric that is based on multiple events.</summary>
```shell
$ bin/pi_generate --metric-only

> You can either count all the events or the number of distinct attributes within events.
> Here are two examples to explain the difference:
> * Count how many times applied_ai_code_review_suggestion was triggered (count)
> * Count the number of distinct projects for which applied_ai_code_review_suggestion was triggered (distinct)
> Would you like to count the total or distinct number of events? count/distinct

distinct

> Please provide the event name that will be used to compute your metric

applied_ai_code_review_suggestion

> Please provide version of applied_ai_code_review_suggestion event for use

1-0-0

> Please specify what attribute would you like to use for uniquness from applied_ai_code_review_suggestion event.
> Available attributes are:
> event_record - to count the total number of event records
> project_id - number, not null
> merge_request_id - number, not null
> merge_request_milestone - string

project_id

> Do you want to add more events Y/N

Y

> Please next event name that will be used to compute your metric

run_ai_security_scan

> Please provide version of run_ai_security_scan event for use

1-0-1

> Please specify what attribute would you like to use for computation from applied_ai_code_review_suggestion event.
> Available attributes are:
> event_record - to count the total number of event records
> project_id - number, not null
> merge_request_id - number, not null
> merge_request_milestone - string

project_id

> Do you want to add more events Y/N

N

> ... Ask about time frame, tier, name and description

```
</details>

The generator (after filling rest of the configuration steps discussed in [_Quick Start_](#quick-start-guide) section) would produce
following metric definition:

<details markdown=1><summary>The metric definition.</summary>

```yaml
---
data_category: optional
name: count_total_users_of_ai_features
description: >
  count_total_users_of_ai_features is representing
  a distinct number of projects that used any of the AI-powered features.
  This metric counts unique project_id from applied_ai_code_review_suggestion unioned with
  unique project_id from run_ai_security_scan
  event records that occurred within the last week and last month.
source: events
events:
- applied_ai_code_review_suggestion:
  version: 1-0-0
  unique_on: project_id
- run_ai_security_scan:
  version: 1-0-1
  unique_on: project_id
filters:
  time_frames: [7days, 28days]
value_type: number
performance_indicator_type: []
introduced_by_url: https://gitlab.com/gitlab-com/www-gitlab-com/-/issues/1234
product_group: group::code review
milestone: "15.10"
tier:
- premium
- ultimate
```

The above configuration would count a number of unique project_id attribute values that occurred for any event records that were recorded for each of the `option.events` listed events within the last week.

</details>

#### Instrumentation

#### Life cycle

### Design

You can use Snowplow events on SaaS to create ad hoc metric without definition in order to explore data and prototype metrics
before creating metrics definitions and including them into reporting from self-managed instances

## Using Usage Insights data

Once you completed all necessary steps to add events and metrics, it is the time to finally look at the resulting data.
This section of documentation is going to discuss what data can you expect and how to access it.

### Event records data

Once your event definition and instrumented event tracking code are deployed to gitlab.com SaaS instance,
event records will start to be captured and stored in data warehouse for future analysis.
You can view event records via SiSense data visualisation tool, by querying data warehouse table that represents your
event, table names are build based on event name and event version, for example:

For event `applied_ai_code_review_suggestion` from [quick start guide](#quick-start-guide) a corresponding table would be named
`applied_ai_code_review_suggestion_1_0_0`, and all event properties would be extracted as separate columns.
An example query that you could use might looks like:

```sql
SELECT
COUNT(DISTINCT project_id)
FROM common.gitlab_pi_events_applied_ai_code_review_suggestion_1_0_0
WHERE recorded_at > DATEADD('days', -7 ,CURRENT_DATE)
```

The above query would return information about number of distinct projects that got at least one AI generated code review suggestions applied in the last 7 days.

Events records data is only available for SaaS (gitlab.com) instance, the only way to get insights based on events from self-managed or dedicated instances is via metrics.

### Metrics data

The same as with the event records data, once you metric definition are deployed to production they will be put to use.
All metrics data is being included into Service Ping payload and reported once a week from all GitLab instances that agreed to
share Analytics Instrumentation data with GitLab organisation.

#### Browsing metric data

All metrics data can be viewed in SiSense by running queries against `common.service_ping_metrics` table in SiSense, eg:

```sql
SELECT SUM(count_distinct_project_id_from_event_applied_ai_code_review_suggestion_v1_0_0_7days)
FROM common.service_ping_metrics
WHERE reported_at > DATEADD('days', -7 ,CURRENT_DATE)
```

The above query returns information about the total number of distinct projects which have applied code review suggestions in the last week.

It is important to notice that a single metric can produce multiple values based on defined `time_frames` where each of the available
time frame aggregations will be suffixed with the name of the time frame.
The metric `count_distinct_project_id_from_event_applied_ai_code_review_suggestion` on version `1-0-0` uses the `7days` and `28days` time frames, therefore it will produce two aggregates:
1. `count_distinct_project_id_from_event_applied_ai_code_review_suggestion_v1_0_0_7days`
1. `count_distinct_project_id_from_event_applied_ai_code_review_suggestion_v1_0_0_28days`

When can you expect to see your metrics:

#### From SaaS (gitlab.com)

The Service Ping report is generated each Monday on a weekly basis, so once you metric definition is deployed to production
it will be included into upcoming Service Ping report on the next Monday after your code is deployed

#### From self-managed instances

The Service Ping process is also run on a weekly basis on self-managed instances, however there are two different cadences that
control how fast your changes would reach self-managed instances:
1. New events are being published as every other change to codebase of GitLab with monthly release cycle,
   and propagates as instances upgrades to the new version which can take multiple months to cover majority of self-managed instances
1. The current list of metrics to be included into Service Ping report is being pulled by each of self-managed instances on a weekly basis.
   However only metrics using events available in a GitLab release running on given instance would be reported

To give some example lets imagine two metrics being added in `15.11` version of GitLab:
1. `count_distinct_authors_closing_issues` that uses the event `close_issue` which was released with `14.11` version
1. `count_ai_security_alerts_viewd` that uses the event `view_ai_security_alert` which was released with `15.11` version

Now consider what metrics would be reported from instance running `15.9` version of GitLab:
1. `count_distinct_authors_closing_issues` will be reported because it uses the event `close_issue` which was released with `14.11` version which is lower then the current instance version `15.9`
1. `count_ai_security_alerts_viewd` will NOT be reported because it uses the event `view_ai_security_alert` which was released with `15.11` version which is higher then the current instance version `15.9`

However if that instance would upgrade to `15.11` version of GitLab all new metrics `count_distinct_authors_closing_issues` and `count_ai_security_alerts_viewd`
would be reported with the next Service Ping report cycle.

### Event dictionary

### Metric dictionary

### Data Visualization

## Migration from legacy systems (Service Ping, Snowplow)

The current system is backward compatible, and all
preexisting data reported from the mentioned system is still going to be reported as it was
{: .alert .alert-info}

In the past Analytics Instrumentation group offered two independent systems for use:
1. [Service Ping](https://docs.gitlab.com/ee/development/service_ping/)
1. Snowplow

Each of those offered different capabilities and was to some extent complementary to each other.
The current system is designed on top of those tools, but mentioned tools got abstracted in a way that the existence of those tools
is internal Analytics Instrumentation group implementation details, and should not concern anybody outside of the group.
The current system combines Service Ping and Snowplow into a single entity to provide the best of both worlds and is expected to replace them both
It is important to notice that **current system is backward compatible**, and **all
preexisting data reported from the mentioned system is still going to be reported as it was**. The current system is going to be applied
only to newly created events and metrics.
